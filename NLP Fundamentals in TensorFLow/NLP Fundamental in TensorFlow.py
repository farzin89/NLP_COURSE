"""NLP Fundamental in Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gnG6K79et9WLTrlNE2C8Rle_29TCis8r

# Introduction to NLP Fundamentals in TensorFlow

NLP has the goal of deriving information out of natural language (could be sequences Text or speech). Another common term for NLP problems is sequence to sequence problems(seq2seq)

## check for GPU
"""

!nvidia-smi =L

"""##Get helper function"""

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

# Import series of helper functions for the notebook

from helper_functions import unzip_data,create_tensorboard_callback,plot_loss_curves,compare_historys



"""## Get a text dataset 

The dataset we're going to be using is Kaggle's introduction to NLP dataset(text samples of Tweeets labelled as disaster or not disaster).

see the original source here: https://www.kaggle.com/competitions/nlp-getting-started/data
"""

!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip

# Unzip data

unzip_data("nlp_getting_started.zip")

"""## Visualizing a text dataset 

To visualize our text samples, we first have to read them in, one way to do so would be to use Python : https://realpython.com/read-write-files-python/

But I prefer to get visual straight away. 
So another way to do this is to use pandas.. 
"""

import pandas as pd

train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")
train_df.head()

train_df["text"][1]

# shuffle training dataframe

train_df_shuffled = train_df.sample(frac=1,random_state=42)
train_df_shuffled.head()

# What does the test dataframe look like?

test_df.head()

# How many examples of each class?
train_df.target.value_counts()

# How many total samples?

len(train_df),len(test_df)

# Let's visualize some random training examples

import random
random_index = random.randint(0,len(train_df)-5) # create random indexs not higher than the total number of samples
for row in train_df_shuffled[["text","target"]][random_index:random_index+ 5 ].itertuples():
  _,text,target = row
  print(f"Target: {target}","(real disaster)" if target > 0 else "(not real disaster)")
  print(f"Text:\n{text}\n")
  print("---\n")


"""### Split data into training and validation sets"""

from sklearn.model_selection import train_test_split

# Use train_test_split to split training data into training and validation sets

train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled["text"].to_numpy(),
                                                                              train_df_shuffled["target"].to_numpy(),
                                                                              test_size=0.1,
                                                                              # use 10% of training data for validation
                                                                              random_state=42)



# Check the lengths

len(train_sentences),len(train_labels),len(val_sentences),len(val_labels)

len(train_df_shuffled)

# check the first 10 sample
train_sentences[:10],train_labels[:10]

"""## Converting text into numbers

Wonderful! We've got a training set and a validation set containing Tweets and labels.

Our labels are in numerical form (0 and 1) but our Tweets are in string form.

###ðŸ¤” Question: What do you think we have to do before we can use a machine learning algorithm with our text data?

If you answered something along the lines of "turn it into numbers", you're correct. A machine learning algorithm requires its inputs to be in numerical form.

In NLP, there are two main concepts for turning text into numbers:

##Tokenization 
- A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:

1.  Using word-level tokenization with the sentence "I love TensorFlow" might result in "I" being 0, "love" being 1 and "TensorFlow" being 2. In this case, every word in a sequence considered a single token.

2.  Character-level tokenization, such as converting the letters A-Z to values 1-26. In this case, every character in a sequence considered a single token.

3.  Sub-word tokenization is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, "my favourite food is pineapple pizza" might become "my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple tokens.

##Embeddings
 Sub-word tokenization is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, "my favourite food is pineapple pizza" might become "my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple tokens.

1. Create your own embedding - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as tf.keras.layers.Embedding) and an embedding representation will be learned during model training.

2. Reuse a pre-learned embedding - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task.

### Text vectorization(tokenization)
"""

train_sentences[:5]

import tensorflow as tf
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
# Note: in TensorFlow 2.6+, you no longer need "layers.experimental.preprocessing"
# you can use: "tf.keras.layers.TextVectorization", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more

# Use the default TextVectorization variables
text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)
                                    standardize="lower_and_strip_punctuation", # how to process text
                                    split="whitespace", # how to split tokens
                                    ngrams=None, # create groups of n-words?
                                    output_mode="int", # how to map tokens to numbers
                                    output_sequence_length=None) # how long should the output sequence of tokens be?
                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None

train_sentences[0].split()
len(train_sentences[0].split())

# find the average number of tokens (words) in the training tweets

round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))

# Setup text vectorization variables
max_vocab_lenght = 10000 # max number of words to have in our vocabulary
max_lenght = 15 # max lenght our sequences will be (e.g how many words from a Tweet does a model see )
text_vectorizer = TextVectorization(max_tokens= max_vocab_lenght,
                                    output_mode ="int",
                                    output_sequence_length = max_lenght)